---
author: "Pontus Svensson (ponsv690) & Kristian Sikiric (krisi211)"
date: ""
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
title: "TDDE07 - Lab 2"
header-includes:
-  \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.pos = "H", out.extra = "", fig.width = 5, fig.height = 5, fig.align = "center", cache = FALSE)
```

# Assignment 1
### Linear and polynomial regression

In this assignent a data set of temperatures in Malmslätt, Linköping was given. The task was to perfrom a Bayesian analysis of a quadratic regression $temp = \beta_0 + \beta_1*time + \beta_2*time^2$. 

First the model parameters for the prior distribution were to be determined. To do this, a conjugate prior for the linear regression model was used. At first we used some given values for the hyperparameters. These resulted in a large variance between different simulations. 

With some trial and error we ended up using the following tuned hyperparameters: $\mu_0 = (-15, 150, -150), \Omega_0 = I_3, \nu_0 = 40, \sigma^2_0 = 1$. This resulted in the following regression curves. 

```{r}
data =read.delim("/home/krisi211/Desktop/TDDE07/Lab2/TempLinkoping.txt")

set.seed(235)

## 1a
Ndraws = 10000
mu_0 = t(data.frame(-15,150,-150))
Omega_0 = 1*diag(3)
nu_0 = 40
var_0 = 1


var<-(nu_0*var_0)/rchisq(Ndraws,nu_0)
#hist(var,freq = FALSE)

beta = matrix(0,Ndraws,3)
library(mvtnorm)

sim.beta = function(sigma2, mu, omega){
  rmvnorm(n=1,mean=mu,sigma=sigma2*solve(omega))
}

betas = sapply(var,sim.beta,mu = mu_0, omega = Omega_0)
betas = t(betas)

reg_fun =function(betas,time){
  temp = betas[1] + betas[2]*time + betas[3]*time^2
  return(temp)
}

prior.temp = apply(betas[seq(1,25,1),],1,reg_fun, time = data$time)
x = dim(prior.temp)[1]
x.axis = (1:x) / x
plot(x.axis,prior.temp[,1],type = "l", xlab = "Time", ylab = "Temperature",ylim=c(-20,30))
for (i in 2:25) {
  lines(x.axis,prior.temp[,i])
}
```

These curves looks reasonable because we see that it is hottest in the middle of the year, i.e during the summer with a temperature around 20. The curves lies roughly around the same temperature, showing that they do not vary too much. Also we see that the mean temperature for the whole year is around 8 degrees, which seems reasonable.

After this the distributions of $\beta_0, \beta_1, \beta_2$ and $\sigma^2$ were to be simulated. Below are the histograms of these distributions aswell as a scatter plot of the temperature data. 

```{r}
## 1b
X = cbind(rep(1,length(data$time)),data$time,data$time^2)
X.prim.X = t(X)%*%X
beta.hat = solve(t(X)%*%X)%*%t(X)%*%data$temp
mu_n = solve((t(X)%*%X + Omega_0))%*%(t(X)%*%X%*%beta.hat + Omega_0%*%mu_0)
Omega_n = X.prim.X + Omega_0
nu_n = nu_0 + dim(X)[1]
nuvar_n = nu_0%*%var_0 + (t(data$temp)%*%data$temp + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n)
var_n = as.numeric(nuvar_n/nu_n)

var.post = (nu_n*var_n)/rchisq(Ndraws,nu_n)
hist(var.post,main = "Histogram of posterior variance", xlab = "")

betas.post = sapply(var.post,sim.beta, mu = mu_n, omega = Omega_n)
betas.post = t(betas.post)

hist(betas.post[,1],main = "Histogram of posterior beta 0", xlab = "")
hist(betas.post[,2],main = "Histogram of posterior beta 1", xlab = "")
hist(betas.post[,3],main = "Histogram of posterior beta 2", xlab = "")

plot(data$temp)
post.temp = apply(betas.post[1:1000,],1,reg_fun,time = data$time)
post.temp.median = apply(post.temp,1,median)
lines(post.temp.median)

post.temp.q = apply(post.temp,1,quantile,probs = c(0.025,0.975),na.rm=TRUE)
lines(post.temp.q[1,],col = "red")
lines(post.temp.q[2,],col = "blue")
legend("bottom", col = c("red", "blue", "black"), legend = c("Lower 2.5%", "Upper 97.5%", "Median"), lty=1)
```

On the scatter plot, the 95 credible interval is plotted around the median. The interval bands does not contain most data points because we look at the median temperature for each day, which is just a single value.

Below we have plotted the expected highest temperature from the distributions simulated above. We can see that the highest temperature is excpected to be between day 180 and 185, i.e in July, which often is the hottest month of the year.

```{r}
## 1c
time = -betas[,2]/(2*betas[,3])
plot(density(time)$x*366,density(time)$y,type="l",xlab = "Days", ylab = "Max Temp dist")
```

If we were to estimate a polynomial model of order 7, but we suspect that higher order terms are not needed, we would use the following specifications:

$\mu_0$: vector of length 7, set the last terms to zero so that the higher order terms have no effect.

$\omega_0$: matrix of dim 7x7, set the last terms in the diagonal to a very high value so that the variance of the higher order terms becomes small.

# Assignment 2
### Posterior approximation for classification with logistic regression

In this assignment a dataset with variables describing parts of women's life situation. This data will be used to predict if a woman works or not.

At first we fitted the logistic regression using maximum likelihood estimation (see code). 

Next we wanted to approximate the posterior distribution of the 8-dim parameter vector $\beta |y,X$~$N(\tilde\beta,J_y^{-1}(\tilde\beta))$.
$\tilde\beta$ and $J(\tilde\beta)$ were calculated using the Optim function in R.

```{r}
data = read.delim("/home/krisi211/Desktop/TDDE07/Lab2/WomenWork.dat",sep="")
#data = read.delim("/home/ponsv690/Documents/TDDE07/Lab2/WomenWork.dat",sep="")
set.seed(235)

## a
glm.model = glm(Work~0 +., data = data, family = binomial)
## b
tau = 10
library("mvtnorm")
y = as.vector(data[,1])
X = as.matrix(data[,2:9])
params = dim(X)[2]

mu = matrix(0,params,1)
Sigma = tau^2*diag(params)

LogPostLogistic = function(betas, y, X, mu, Sigma) {
  params = length(betas)
  linPred = X%*%betas
  
  logLik = sum(linPred*y -log(1+exp(linPred)))
  if (abs(logLik) == Inf) logLik = -20001
  
  logPrior = dmvnorm(betas, mu, Sigma, log=TRUE)
  
  return(logLik + logPrior)
}

initVal = c(rep(0,dim(X)[2]))

OptimResults = optim(initVal,LogPostLogistic,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)

beta.tilde = OptimResults$par
inv.hessian = -solve(OptimResults$hessian)

beta = rmvnorm(10000,beta.tilde,inv.hessian)

#quantile(beta[,7],probs = c(0.025,0.975))
CI.lo = beta.tilde[7] + 1.96*(inv.hessian[7,7])^0.5
CI.hi = beta.tilde[7] - 1.96*(inv.hessian[7,7])^0.5
```

The numerical values for $\tilde\beta$ is
```{r}
paste((beta.tilde), collapse = " ")
```

and for $J_y^{-1}(\tilde\beta)$ is

```{r}
write.table(round(inv.hessian,digits = 5), row.names = FALSE, col.names = FALSE, sep = "")
```









